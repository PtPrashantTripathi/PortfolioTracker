{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary files and packages\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from StockETL.globalpath import GlobalPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuations from the columns\n",
    "def replace_punctuation_from_string(input_str):\n",
    "    \"\"\"replace punctuation from string Function\"\"\"\n",
    "    regex_escape_string = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^`{|}~\"\"\"\n",
    "    regex_remove_punctuation = re.compile(\"[%s]\" % re.escape(regex_escape_string))\n",
    "    output_str = (\n",
    "        regex_remove_punctuation.sub(\"\", str(input_str))\n",
    "        .strip()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"\\n\", \"_\")\n",
    "        .replace(\"\\t\", \"_\")\n",
    "        .replace(\"\\r\", \"_\")\n",
    "        .lower()\n",
    "    )\n",
    "    while \"__\" in output_str:\n",
    "        output_str = output_str.replace(\"__\", \"_\")\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punctuation_from_columns(df_pandas):\n",
    "    \"\"\"Pandas version of replace punctuation Function\"\"\"\n",
    "    new_col_names = []\n",
    "    for col_name in df_pandas.columns:\n",
    "        new_col_name = replace_punctuation_from_string(col_name)\n",
    "        new_col_names.append(new_col_name)\n",
    "    df_pandas.columns = new_col_names\n",
    "    return df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix duplicate column names in a Pandas DataFrame\n",
    "def fix_duplicate_column_names(df_pandas):\n",
    "    \"\"\"\n",
    "    This function receives a Pandas DataFrame and ensures that each column name is unique.\n",
    "    If a duplicate name is found, the function renames it by appending an incremental number, e.g. '_1', '_2', etc.\n",
    "    The function returns a new DataFrame with the updated column names.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    counts = {}\n",
    "    for column_name in df_pandas.columns:\n",
    "        column_name = replace_punctuation_from_string(str(column_name))\n",
    "        if column_name not in counts:\n",
    "            counts[column_name] = 0\n",
    "            result.append(column_name)\n",
    "        else:\n",
    "            counts[column_name] += 1\n",
    "            result.append(f\"{column_name}_{counts[column_name]}\")\n",
    "    df_pandas.columns = result\n",
    "\n",
    "    if len(result) == 0:\n",
    "        raise ValueError(\"Duplicate column issue!\")\n",
    "    else:\n",
    "        return df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions to gather debug of given pandas dataframe\n",
    "def find_correct_sheetname(df_pandas, sheet_name_regex):\n",
    "    \"\"\"\n",
    "    Finds the first sheet name that matches the given regular expression.\n",
    "\n",
    "    Parameters:\n",
    "    df_pandas (dict): A dictionary where keys are sheet names and values are the corresponding data frames.\n",
    "    sheet_name_regex (str): A regular expression pattern to match against the sheet names.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The data frame corresponding to the first sheet name that matches the regex.\n",
    "    \"\"\"\n",
    "    # Compile the regular expression for efficiency\n",
    "    pattern = re.compile(sheet_name_regex, re.IGNORECASE)\n",
    "\n",
    "    # Iterate through the sheet names\n",
    "    for sheet_name in df_pandas.keys():\n",
    "        # Check if the sheet name matches the regex pattern\n",
    "        if pattern.search(sheet_name):\n",
    "            print(f\"Sheet name => {sheet_name}\")\n",
    "            return df_pandas[sheet_name]\n",
    "\n",
    "    # Raise an error if no matching sheet name is found\n",
    "    raise ValueError(\"Sheet name not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to find data with correct header column\n",
    "def find_correct_headers(df_pandas, global_header_regex=None):\n",
    "    \"\"\"\n",
    "    Auxiliary functions to gather debug of given pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = re.compile(global_header_regex, re.IGNORECASE)\n",
    "    # Iterate through the pandas data\n",
    "    for header_row_index, row in df_pandas.iterrows():\n",
    "        for each in row.values:\n",
    "            # Check if the sheet name matches the regex pattern\n",
    "            if pattern.match(replace_punctuation_from_string(str(each))):\n",
    "                df = df_pandas.iloc[header_row_index + 1 :]\n",
    "                df.columns = df_pandas.iloc[header_row_index]\n",
    "                # drop col which are all null\n",
    "                # df = df.dropna(axis=1, how=\"all\")\n",
    "                return df\n",
    "    raise ValueError(\"Header not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_files_availability(\n",
    "    dir_path: Union[str, Path],\n",
    "    file_pattern: str = \"*\",\n",
    "    timestamp: datetime = datetime.strptime(\"2000-01-01\", \"%Y-%m-%d\"),\n",
    ") -> List[Path]:\n",
    "    \"\"\"\n",
    "    Checks for newly added or modified files in a directory after a specific timestamp.\n",
    "\n",
    "    Args:\n",
    "        dir_path (Union[str, Path]): The directory to check for files.\n",
    "        file_pattern (str): The pattern to filter files.\n",
    "        timestamp (datetime): The timestamp to compare file modification times against.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of paths to files that were added or modified after the given timestamp.\n",
    "    \"\"\"\n",
    "    # List to store paths of matched files\n",
    "    file_paths = []\n",
    "\n",
    "    # Iterate over all files in the directory and subdirectories\n",
    "    for file_path in Path(dir_path).rglob(file_pattern):\n",
    "        if file_path.is_file():\n",
    "            file_modified_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "            # Check if file was modified after the given timestamp\n",
    "            if file_modified_time > timestamp:\n",
    "                file_paths.append(file_path)\n",
    "\n",
    "    # Log the number of detected files\n",
    "    num_files = len(file_paths)\n",
    "    if num_files > 0:\n",
    "        print(f\"Number of Files Detected => {num_files}\")\n",
    "        return file_paths\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No processable data available in the directory: {file_path}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_with_datacontract(\n",
    "    df: pd.DataFrame, data_contract_path: GlobalPath, rounding=True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aligns the DataFrame with the DataContract specified in a JSON file.\n",
    "    This function casts DataFrame columns to the data types specified in the schema,\n",
    "    creates missing columns with the correct data type, and arranges the columns in order.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to align.\n",
    "        data_contract_path (GlobalPath): Path to the JSON file containing DataContract information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame aligned with the DataContract.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load DataContract from the JSON file\n",
    "    with open(data_contract_path, encoding=\"utf-8\") as schema_file:\n",
    "        datacontract = json.load(schema_file)\n",
    "        # print(f\"DataContract loaded from => {data_contract_path}\")\n",
    "\n",
    "    # Extract schema definitions and column order from the JSON\n",
    "    data_schema = datacontract.get(\"data_schema\", [])\n",
    "    order_by = datacontract.get(\"order_by\", [])\n",
    "\n",
    "    # Iterate over the schema to align DataFrame columns\n",
    "    for col_info in data_schema:\n",
    "        col_name = col_info[\"col_name\"]\n",
    "        col_type = col_info[\"data_type\"]\n",
    "\n",
    "        if col_name in df.columns:\n",
    "            # Cast column to the specified data type\n",
    "            df[col_name] = df[col_name].astype(col_type)\n",
    "        else:\n",
    "            # Create missing column with NaN values and specified data type\n",
    "            df[col_name] = pd.Series([None] * len(df), dtype=col_type)\n",
    "\n",
    "    # Ensure select columns specified by the schema\n",
    "    all_columns = [each[\"col_name\"] for each in data_schema]\n",
    "    df = df[all_columns]\n",
    "\n",
    "    # Reorder DataFrame columns according to the order specified by the schema\n",
    "    order_by = list(dict.fromkeys(order_by + all_columns))\n",
    "    df = df.sort_values(by=order_by).reset_index(drop=True)\n",
    "\n",
    "    # Round numerical values to 2 decimal places\n",
    "    if rounding:\n",
    "        df = df.round(2)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to DataContract\n",
    "def get_correct_datatype(input_datatype):\n",
    "    input_datatype = str(input_datatype).lower().strip()\n",
    "    datatypes_list = {\n",
    "        \"Date\": [\"date\"],\n",
    "        \"string\": [\"string\", \"varchar\", \"char\", \"text\", \"object\"],\n",
    "        \"Long\": [\"bigint\", \"int\", \"tinyint\", \"long\"],\n",
    "        \"Timestamp\": [\"timestamp\", \"datetime\"],\n",
    "        \"Double\": [\"double\", \"float\", \"decimal\"],\n",
    "        \"Boolean\": [\"bool\", \"boolean\"],\n",
    "    }\n",
    "    for datatype_name, datatype_values in datatypes_list.items():\n",
    "        if input_datatype in datatype_values:\n",
    "            return datatype_name\n",
    "    print(f\"undefined data type => {input_datatype}\")\n",
    "    return input_datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataType\tDescription\n",
    "# date\tStore Date Only\n",
    "# datetime\tStore Date and Time\n",
    "# string\tString or character\n",
    "# int\tRepresents 4-byte signed integer numbers. The range of numbers is from -2147483648 to 2147483647.\n",
    "# long\tRepresents 8-byte signed integer numbers. The range of numbers is from -9223372036854775808 to 9223372036854775807\n",
    "# bit\tbit\n",
    "# timestamp\tdate with time detail in ISOformat\n",
    "# double\tRepresents 8-byte double-precision floating point numbers\n",
    "# float\tRepresents 4-byte single-precision floating point numbers.\n",
    "# boolean\tTrue or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_contract(df, schema_path):\n",
    "    # Create the list of dictionaries\n",
    "    data_schema = [\n",
    "        {\n",
    "            \"col_name\": col,\n",
    "            \"data_type\": str(dtype),  # get_correct_datatype(str(dtype)),\n",
    "        }\n",
    "        for col, dtype in df.dtypes.to_dict().items()\n",
    "    ]\n",
    "    # Write the dictionary to a JSON file\n",
    "    with open(schema_path, \"w\") as json_file:\n",
    "        json.dump({\"data_schema\": data_schema}, json_file, indent=4)\n",
    "\n",
    "    print(f\"Dictionary written to {schema_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with empty strings in a DataFrame\n",
    "def replace_nan_with_empty(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {key: replace_nan_with_empty(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [replace_nan_with_empty(item) for item in data]\n",
    "    elif isinstance(data, float) and np.isnan(data):\n",
    "        return \"\"\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
