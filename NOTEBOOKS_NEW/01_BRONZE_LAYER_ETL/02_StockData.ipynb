{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOURCE TO BRONZE LAYER\n",
    "\n",
    "### Process:\n",
    "\n",
    "> The function fetches daily stock data using the Yahoo Finance API (`yfinance`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from datetime import timedelta, datetime\n",
    "from pyrate_limiter import Duration, Limiter, RequestRate\n",
    "from requests_ratelimiter import LimiterMixin, MemoryQueueBucket\n",
    "from requests_cache import CacheMixin, SQLiteCache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from StockETL import DateTimeUtil, GlobalPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Common Utility Function\n",
    "%run ../COMMON/common_utility.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "stock_tickers_config_path = GlobalPath(\"CONFIG/CONSTANTS/stock_tickers.json\")\n",
    "holding_history_path = GlobalPath(\"DATA/SOURCE/Holding/Holding_data.csv\")\n",
    "stockdata_bronze_layer_path = GlobalPath(\"DATA/BRONZE/StockData\")\n",
    "stockdata_bronze_schema_file_path = GlobalPath(\n",
    "    \"CONFIG/DATA_CONTRACTS/BRONZE/StockData.json\"\n",
    ")\n",
    "failed_records_path = GlobalPath(\"DATA/FAILED/failed_records.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up rate limiter\n",
    "class CachedLimiterSession(CacheMixin, LimiterMixin, requests.Session):\n",
    "    pass\n",
    "\n",
    "\n",
    "history_rate = RequestRate(1, Duration.SECOND * 2)\n",
    "limiter = Limiter(history_rate)\n",
    "session = CachedLimiterSession(\n",
    "    limiter=limiter,\n",
    "    bucket_class=MemoryQueueBucket,\n",
    "    backend=SQLiteCache(\".cache/session\", expire_after=timedelta(hours=1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_github(output_file):\n",
    "    github_data_url = f\"https://raw.githubusercontent.com/PtPrashantTripathi/PortfolioTracker/main/DATA/BRONZE/StockData/{output_file.name}\"\n",
    "    response = requests.get(github_data_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_file, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "            return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for stock ticker overrides\n",
    "# Open and read the JSON file\n",
    "OVERWRITE_TICKERS = {}\n",
    "with open(stock_tickers_config_path, encoding=\"utf-8\") as f:\n",
    "    # Get the contract_fields from json data\n",
    "    OVERWRITE_TICKERS = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download data\n",
    "def process_stock_data(row):\n",
    "    failed_records = []\n",
    "    try:\n",
    "        stock_ticker = yf.Ticker(\n",
    "            OVERWRITE_TICKERS.get(row[\"symbol\"], row[\"isin\"]),\n",
    "            session=session,\n",
    "        )\n",
    "        date_list = generate_date_list(\n",
    "            row[\"min_date\"].to_pydatetime(), row[\"max_date\"].to_pydatetime()\n",
    "        )\n",
    "\n",
    "        for date in date_list:\n",
    "            output_file = stockdata_bronze_layer_path.joinpath(\n",
    "                f\"{row['symbol']}_{date.year:04d}_{date.month:02d}.csv\"\n",
    "            )\n",
    "            if (\n",
    "                output_file.exists()\n",
    "                and date.month_difference(DateTimeUtil.today()) >= 1\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            if download_file_from_github(output_file):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df = stock_ticker.history(\n",
    "                    start=date.start_date,\n",
    "                    end=min(date.end_date, DateTimeUtil.today()),\n",
    "                    interval=\"1d\",\n",
    "                    actions=True,\n",
    "                    rounding=True,\n",
    "                )\n",
    "                \n",
    "                if df.empty:\n",
    "                    raise ValueError(\"No data returned\")\n",
    "\n",
    "                df = df.reset_index()\n",
    "\n",
    "                # Replace punctuation from column names for consistency\n",
    "                df = replace_punctuation_from_columns(df)\n",
    "\n",
    "                # Fix duplicate column names by appending numerical suffixes\n",
    "                df = fix_duplicate_column_names(df)\n",
    "\n",
    "                # Drop rows where all elements are NaN\n",
    "                df = df.dropna(how=\"all\")\n",
    "\n",
    "                # Align Datafame with DataContract\n",
    "                df = align_with_datacontract(df, stockdata_bronze_schema_file_path)\n",
    "\n",
    "                df.to_csv(output_file, index=False)\n",
    "                print(f\"Saved: {output_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                failed_records.append(\n",
    "                    {\n",
    "                        \"symbol\": row[\"symbol\"],\n",
    "                        \"date_range\": f\"{date.start_date} to {date.end_date}\",\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        failed_records.append({\"symbol\": row[\"symbol\"], \"error\": str(e)})\n",
    "\n",
    "    return failed_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load holding history\n",
    "df_holding_history = pd.read_csv(holding_history_path)\n",
    "df_holding_history[\"min_date\"] = pd.to_datetime(df_holding_history[\"min_date\"])\n",
    "df_holding_history[\"max_date\"] = pd.to_datetime(df_holding_history[\"max_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process in parallel\n",
    "failed_records = []\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = executor.map(\n",
    "        process_stock_data, df_holding_history.to_dict(orient=\"records\")\n",
    "    )\n",
    "    for result in results:\n",
    "        failed_records.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save failed records\n",
    "pd.DataFrame(failed_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_debug": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
